# Packages
import numpy as np
from matplotlib import pyplot as plt
from scipy import integrate

# Constants
c = 299792.458 # Speed of light in km/s
H0kmsmpc = 70.  # Hubble constant in km/s/Mpc
H0s = H0kmsmpc * 3.2408e-20 # H0 in 1/s

# Write a function for the integrand
def ezinv(z,om=0.3,ol=0.7,w0=-1.0,wa=0.0,orr=0.0):
    ok = 1.-om-ol-orr
    ez = np.sqrt(om * (1+z)**3 + ok *(1+z)**2 + ol)
    return 1./ez

# The curvature correction function
def Sk(xx, ok):
    if ok < 0.0:
        dk = np.sin(np.sqrt(-ok)*xx)/np.sqrt(-ok)
    elif ok > 0.0:
        dk = np.sinh(np.sqrt(ok)*xx)/np.sqrt(ok)
    else:
        dk = xx
    return dk

# The distance modulus
def dist_mod(zs,om=0.3,ol=0.7,w0=-1.0,wa=0.0,orr=0.0):
    ok = 1.0 - om - ol
    xx = np.array([integrate.quad(ezinv, 0, z, args=(om, ol, w0, wa, orr))[0] for z in zs])
    D = Sk(xx, ok)
    lum_dist = D * (1 + zs) 
    dist_mod = 5 * np.log10(lum_dist) 
    dist_mod = dist_mod + np.log(c/H0kmsmpc)-(-19.5) 
    return dist_mod
  
# Read in data
import os
data = '/Users/erinsullivan/Downloads/data/'

def read_data(model_name):
    d = np.genfromtxt(data+model_name+'.txt',delimiter=',')
    zs = d[:,0]
    mu = d[:,1]
    muerr=d[:,2]
    return zs, mu, muerr

zs, mu, muerr = read_data('Data1')

# Plotting Hubble diagram
plt.errorbar(zs, mu, yerr=muerr, fmt='o',capsize=2, markersize=2, alpha=0.5, color='black', ecolor='palevioletred')
plt.xlim(0,1.0)
plt.xlabel('Redshift $z$')
plt.ylabel('Magnitude')
plt.show()

# Empty model as a benchmark to compare the others to
mu_om00_ox00 = dist_mod(zs,om=0.0,ol=0.0)  
mu_om10_ox00 = dist_mod(zs,om=1.0,ol=0.0)
mu_om03_ox00 = dist_mod(zs,om=0.3,ol=0.0)
mu_om03_ox07 = dist_mod(zs,om=0.3,ol=0.7)

# Plotting
plt.errorbar(zs,mu,yerr=muerr,fmt='.',elinewidth=0.7,markersize=4,alpha=0.5)
plt.plot(zs,mu_om00_ox00,':',color='black',label='(0.0, 0.0)')
plt.plot(zs,mu_om10_ox00,'-.',color='red',label='(1.0, 0.0)')
plt.plot(zs,mu_om03_ox00,'--',color='blue',label='(0.3, 0.0)')
plt.plot(zs,mu_om03_ox07,'-',color='green',label='(0.3, 0.7)')
plt.xlim(0,1.0)
plt.xlabel('Redshift $z$')
plt.ylabel('Magnitude')
plt.legend(frameon=False)
plt.show()

# Plotting a Hubble diagram relative to the empty model 
plt.errorbar(zs,mu-mu_om00_ox00,yerr=muerr,fmt='.',elinewidth=0.7,markersize=4,alpha=0.5)
plt.plot(zs,mu_om10_ox00-mu_om00_ox00,'-.',color='red',label='(1.0, 0.0)')
plt.plot(zs,mu_om03_ox00-mu_om00_ox00,'--',color='blue',label='(0.3, 0.0)')
plt.plot(zs,mu_om03_ox07-mu_om00_ox00,'-',color='green',label='(0.3, 0.7)')
plt.axhline(y=0.0,ls=':',color='black')
plt.xlim(0.0,1.0)
plt.xlabel('Redshift $z$')
plt.ylabel('Magnitude normalised to (0,0)')
plt.legend(frameon=False)
plt.show()

# Calculate mscript for each of these
mscr_om10_ox00 = np.sum((mu_om10_ox00-mu)/muerr**2)/np.sum(1./muerr**2)
mscr_om03_ox00 = np.sum((mu_om03_ox00-mu)/muerr**2)/np.sum(1./muerr**2)
mscr_om03_ox07 = np.sum((mu_om03_ox07-mu)/muerr**2)/np.sum(1./muerr**2)

print(mscr_om10_ox00,mscr_om03_ox00,mscr_om03_ox07)

mu_om10_ox00=mu_om10_ox00-mscr_om10_ox00
mu_om03_ox00=mu_om03_ox00-mscr_om03_ox00
mu_om03_ox07=mu_om03_ox07-mscr_om03_ox07

# Repeat the plot and see how it changes
plt.errorbar(zs,mu,yerr=muerr,fmt='.',elinewidth=0.7,markersize=4,alpha=0.5)
plt.plot(zs,mu_om10_ox00,'-.',color='red',label='(1.0, 0.0)')
plt.plot(zs,mu_om03_ox00,'--',color='blue',label='(0.3, 0.0)')
plt.plot(zs,mu_om03_ox07,'-',color='green',label='(0.3, 0.7)')
plt.xlim(0,1.0)
plt.xlabel('Redshift $z$')
plt.ylabel('Magnitude')
plt.legend(frameon=False)
plt.show()

# Plotting a Hubble diagram relative to the empty model 
plt.errorbar(zs,mu-mu_om00_ox00,yerr=muerr,fmt='.',elinewidth=0.7,markersize=4,alpha=0.5)
plt.plot(zs,mu_om10_ox00-mu_om00_ox00,'-.',color='red',label='(1.0, 0.0)')
plt.plot(zs,mu_om03_ox00-mu_om00_ox00,'--',color='blue',label='(0.3, 0.0)')
plt.plot(zs,mu_om03_ox07-mu_om00_ox00,'-',color='green',label='(0.3, 0.7)')
plt.axhline(y=0.0,ls=':',color='black')
plt.xlim(0,1.0)
plt.xlabel('Redshift $z$')
plt.ylabel('Magnitude normalised to (0,0)')
plt.legend(frameon=False)
plt.show()


# Set up the arrays for the models you want to test, e.g. a range of Omega_m and Omega_Lambda models:
n = 101                          # Increase this for a finer grid
oms = np.linspace(0.0, 0.5, n)   # Array of matter densities
ols = np.linspace(0.0, 1.0, n)   # Array of cosmological constant values
chi2 = np.ones((n, n)) * np.inf  # Array to hold our chi2 values, set initially to super large values

# Calculate Chi2 for each model
for i, om in enumerate(oms):                                          # loop through matter densities
        for j, ol in enumerate(ols):                                  # loop through cosmological constant densities
            mu_model = dist_mod(zs, om=om, ol=ol)                     # calculate the distance modulus vs redshift for that model 
            mscr = np.sum((mu_model-mu)/muerr**2)/np.sum(1./muerr**2) # Calculate the vertical offset to apply
            mu_model_norm = mu_model-mscr                             # Apply the vertical offset
            chi2[i,j] = np.sum((mu_model_norm - mu) ** 2 / muerr**2)  # Calculate the chi2 and save it in a matrix
            
# Convert that to a likelihood and calculate the reduced chi2
likelihood = np.exp(-0.5 * (chi2-np.amin(chi2)))  # convert the chi^2 to a likelihood (np.amin(chi2) calculates the minimum of the chi^2 array)
chi2_reduced = chi2 / (len(mu)-2)                 # calculate the reduced chi^2, i.e. chi^2 per degree of freedom, where dof = number of data points minus number of parameters being fitted 

# Calculate the best fit values (where chi2 is minimum)
indbest = np.argmin(chi2)                 # Gives index of best fit but where the indices are just a single number
ibest   = np.unravel_index(indbest,[n,n]) # Converts the best fit index to the 2d version (i,j)
print( 'Best fit values are (om,ol)=(%.3f,%.3f)'%( oms[ibest[0]], ols[ibest[1]] ) )
print( 'Reduced chi^2 for the best fit is %0.2f'%chi2_reduced[ibest[0],ibest[1]] )


import matplotlib as mpl

# Define contour levels and colors
levels = [2.30, 6.18, 11.83]
colors = ['palevioletred', 'plum', 'black']

# Create colormap and norm
cmap = mpl.colors.ListedColormap(colors)
norm = mpl.colors.BoundaryNorm(levels, cmap.N)

# Plot contours
contours = plt.contour(oms, ols, np.transpose(chi2-np.amin(chi2)), cmap=cmap, levels=levels)
plt.clabel(contours, inline=True, fontsize=10, fmt={levels[0]: r'$\sigma_{1}$', levels[1]: r'$\sigma_{2}$', levels[2]: r'$\sigma_{3}$'})

# Plot best fit point
plt.plot(oms[ibest[0]], ols[ibest[1]], 'o', color='black', label='($\Omega_m$,$\Omega_\Lambda$)=(%.3f,%.3f)'%(oms[ibest[0]], ols[ibest[1]]))

# Set axis labels and legend
plt.xlabel("$\Omega_m$", fontsize=12)
plt.ylabel("$\Omega_\Lambda$", fontsize=12)
plt.legend(frameon=False, loc='lower right')

# Show the plot and close the figure
plt.show()
plt.close()




# Priors 

# CMB prior 
n = 101                          # Increase this for a finer grid
oms = np.linspace(0.0, 0.5, n)   # Array of matter densities
ols = 1.00 - oms                 # Array of cosmological constant densities assuming flatness
chi2 = np.ones((n, n)) * np.inf  # Array to hold our chi2 values, set initially to super large values

# Define the prior information
p_prior = 1.00       # Prior value of Omega_M + Omega_L
sigma_prior = 0.05   # Prior uncertainty of Omega_M + Omega_L

# Calculate Chi2 for each model
for i, om in enumerate(oms):                                                      # Loop through matter densities
    for j, ol in enumerate(ols):                                                  # Loop through cosmological constant densities
        p = om + ol                                                               # Calculate the parameter  Omega_M + Omega_L
        prior_term = ((p - p_prior) / sigma_prior) ** 2                           # Calculate the prior term
        mu_model = dist_mod(zs, om=om, ol=ol)                                     # Calculate the distance modulus vs redshift for that model 
        mscr = np.sum((mu_model - mu) / muerr ** 2) / np.sum(1. / muerr ** 2)     # Calculate the vertical offset to apply
        mu_model_norm = mu_model - mscr                                           # Apply the vertical offset
        chi2[i, j] = np.sum((mu_model_norm - mu) ** 2 / muerr ** 2) + prior_term  # Calculate the chi2 with the prior term

# Convert that to a likelihood and calculate the reduced chi2
likelihood = np.exp(-0.5 * (chi2 - np.amin(chi2)))  # Convert the chi^2 to a likelihood (np.amin(chi2) calculates the minimum of the chi^2 array)
chi2_reduced = chi2 / (len(mu) - 2)                 # Calculate the reduced chi^2, i.e. chi^2 per degree of freedom, where dof = number of data points minus number of parameters being fitted

# Calculate the best fit values (where chi2 is minimum)
indbest = np.argmin(chi2)                 # Gives index of best fit but where the indices are just a single number
ibest = np.unravel_index(indbest, [n, n]) # Converts the best fit index to the 2d version (i,j)
print('Best fit values are (om,ol)=(%.3f,%.3f)' % (oms[ibest[0]], ols[ibest[1]]))
print('Reduced chi^2 for the best fit is %0.2f' % chi2_reduced[ibest[0], ibest[1]])



# BDO prior

# Set up the arrays for the models you want to test, e.g. a range of Omega_m and Omega_Lambda models:
n = 101                            # Increase this for a finer grid
oms = np.linspace(0.25, 0.35, n)   # Array of matter densities
ols = np.linspace(0.0, 1.0, n)     # Array of cosmological constant values
chi2 = np.ones((n, n)) * np.inf    # Array to hold our chi2 values, set initially to super large values

# Define the prior information
om_prior = 0.30       # Prior value of Omega_M 
sigma_prior = 0.05   # Prior uncertainty of Omega_M 

# Calculate Chi2 for each model
for i, om in enumerate(oms):                                                      # Loop through matter densities
    for j, ol in enumerate(ols):                                                  # Loop through cosmological constant densities
        prior_term = ((om - om_prior) / sigma_prior) ** 2                         # Calculate the prior term
        mu_model = dist_mod(zs, om=om, ol=ol)                                     # Calculate the distance modulus vs redshift for that model 
        mscr = np.sum((mu_model - mu) / muerr ** 2) / np.sum(1. / muerr ** 2)     # Calculate the vertical offset to apply
        mu_model_norm = mu_model - mscr                                           # Apply the vertical offset
        chi2[i, j] = np.sum((mu_model_norm - mu) ** 2 / muerr ** 2) + prior_term  # Calculate the chi2 with the prior term

# Convert that to a likelihood and calculate the reduced chi2
likelihood = np.exp(-0.5 * (chi2 - np.amin(chi2)))  # Convert the chi^2 to a likelihood (np.amin(chi2) calculates the minimum of the chi^2 array)
chi2_reduced = chi2 / (len(mu) - 2)                 # Calculate the reduced chi^2, i.e. chi^2 per degree of freedom, where dof = number of data points minus number of parameters being fitted

# Calculate the best fit values (where chi2 is minimum)
indbest = np.argmin(chi2)                 # Gives index of best fit but where the indices are just a single number
ibest = np.unravel_index(indbest, [n, n]) # Converts the best fit index to the 2d version (i,j)
print('Best fit values are (om,ol)=(%.3f,%.3f)' % (oms[ibest[0]], ols[ibest[1]]))
print('Reduced chi^2 for the best fit is %0.2f' % chi2_reduced[ibest[0], ibest[1]])








# Distances

# Calculate luminosity distance and angular diameter distance
def lum_dist(zs, om = 0.130, ol = 0.680, w0=-1.0, wa=0.0, orr=0.0):
    ok = 1.0 - om - ol
    xx = np.array([integrate.quad(ezinv, 0, z, args=(om, ol, w0, wa, orr))[0] for z in zs])
    D = Sk(xx, ok)
    DL = D * (1 + zs)
    return DL


# Compute error in luminosity distance
DL = lum_dist(zs)
LDerr = abs(DL * np.log(10) * muerr / 5)

def ang_diam_dist(zs, om = 0.130, ol = 0.680, w0=-1.0, wa=0.0, orr=0.0):
    ok = 1.0 - om - ol
    xx = np.array([integrate.quad(ezinv, 0, z, args=(om, ol, w0, wa, orr))[0] for z in zs])
    D = Sk(xx, ok)
    DA = D / (1 + zs)
    return DA

# Compute error in angular diameter distance
DA = ang_diam_dist(zs)
ADerr =abs(DA * 2 * LDerr / (1 + zs) ** 2)


# Plot luminosity distance versus redshift
fig, ax = plt.subplots()
ax.errorbar(zs, lum_dist(zs), yerr=LDerr, fmt='o', color='black', capsize=3, markersize=2, alpha=0.5, ecolor='palevioletred')
ax.set_xlabel('Redshift z')
ax.set_ylabel('Luminosity Distance (Mpc)')
plt.xlim([0, 1])
plt.show()

# Plot angular diameter distance versus redshift
fig, ax = plt.subplots()
ax.errorbar(zs, ang_diam_dist(zs), yerr=ADerr, fmt='o', color='black', capsize=3, markersize=2, alpha=0.5, ecolor='palevioletred')
ax.set_xlabel('Redshift z')
ax.set_ylabel('Angular Diameter Distance (Mpc)')
plt.xlim([0, 1])
plt.show()



# Data 3

# 3 parameter analysis 

# Write a function for the integrand
def ezinv(z, om=0.3, w0=-1, wa=0.0):
    a = 1/(1+z)
    w = w0 + wa*(1-a) # Dark energy equation of state
    ez = np.sqrt( om*(1+z)**3 + (1 - om)*(1+z)**( 3*(1+w) ) )
    return 1./ez


# The curvature correction function
def Sk(xx, ok):
    if ok < 0.0:
        dk = np.sin(np.sqrt(-ok)*xx)/np.sqrt(-ok)
    elif ok > 0.0:
        dk = np.sinh(np.sqrt(ok)*xx)/np.sqrt(ok)
    else:
        dk = xx
    return dk


# The distance modulus
def dist_mod(zs,om=0.3,w0=-1,wa=0.0):
    """ Calculate the distance modulus, correcting for curvature"""
    ok = 0
    xx = np.array([integrate.quad(ezinv, 0, z, args=(om, w0, wa))[0] for z in zs])
    D = Sk(xx, ok)
    lum_dist = D * (1 + zs) 
    dist_mod = 5 * np.log10(lum_dist) # Distance modulus
    dist_mod = dist_mod + np.log(c/H0kmsmpc)-(-19.5) 
    return dist_mod


# Set up the arrays for the models you want to test, including the range of Omega_m, Omega_Lambda, and wa models:
n = 25                                     # Increase this for a finer grid
oms = np.linspace(0.0, 0.5, n)           # Array of matter densities
w0s = np.linspace(-0.8, -0.3, n)           # Array of cosmological constant values
was = np.linspace(-0.7, -0.3, n)              # Array of wa values
chi2 = np.ones((n, n, n)) * np.inf         # Array to hold our chi2 values, set initially to super large values


# Calculate Chi2 for each model
for i, om in enumerate(oms):                                            # Loop through matter densities
    for j, w0 in enumerate(w0s):                                        # Loop through cosmological constant densities
        for k, wa in enumerate(was):                                    # Loop through wa values
            mu_model = dist_mod(zs, om=om, w0=w0, wa=wa)                # Calculate the distance modulus vs redshift for that model 
            mscr = np.sum((mu_model-mu)/muerr**2)/np.sum(1./muerr**2)   # Calculate the vertical offset to apply
            mu_model_norm = mu_model - mscr                             # Apply the vertical offset
            chi2[i, j, k] = np.sum((mu_model_norm - mu)**2 / muerr**2)  # Calculate the chi2 and save it in a matrix
            

# Convert that to a likelihood and calculate the reduced chi2
likelihood = np.exp(-0.5 * (chi2 - np.amin(chi2))) 
chi2_reduced = chi2 / (len(mu) - 3)                

# Calculate the best fit values (where chi2 is minimum)
indbest = np.argmin(chi2)                      # Gives index of best fit but where the indices are just a single number
ibest = np.unravel_index(indbest, [n, n, n])   # Converts the best fit index to the 3d version (i, j, k)
print('Best fit values are (om, w0, wa) = (%.3f, %.3f, %.3f)' % (oms[ibest[0]], w0s[ibest[1]], was[ibest[2]]))
print('Reduced chi^2 for the best fit is %0.2f' % chi2_reduced[ibest[0], ibest[1], ibest[2]])

# Best fit parameters 
indbest = np.argmin(chi2)
ibest = np.unravel_index(indbest, chi2.shape)
best_om = oms[ibest[0]]
best_w0 = w0s[ibest[1]]
best_wa = was[ibest[2]]

# Calculate delta chi square values
delta_chi2 = chi2 - np.min(chi2)

# Define the confidence level 
confidence_level = 0.68

# Find the contour level corresponding to the confidence level
contour_level = scipy.stats.chi2.ppf(confidence_level, df=3)  # df=3 for 3 parameters

# Identify the parameter ranges within the contour level
indices = np.where(delta_chi2 <= contour_level)
om_range = oms[indices[0]]
w0_range = w0s[indices[1]]
wa_range = was[indices[2]]

# Calculate the uncertainties
om_error = (np.max(om_range) - np.min(om_range))/2
w0_error = (np.max(w0_range) - np.min(w0_range))/2
wa_error = (np.max(wa_range) - np.min(wa_range))/2


# Find radiation

# Redefine the function for the integrand with radiation
def ezinv(z, om=0.3, w0=-1, wa=0.0, orr = 0.0):
    a = 1/(1+z)
    w = w0 + wa*(1-a) # Dark energy equation of state
    ez = np.sqrt( om*(1+z)**3 + (1 - om)*(1+z)**( 3*(1+w) ) + orr*(1+z)**4 )
    return 1./ez

# The curvature correction function
def Sk(xx, ok):
    if ok < 0.0:
        dk = np.sin(np.sqrt(-ok)*xx)/np.sqrt(-ok)
    elif ok > 0.0:
        dk = np.sinh(np.sqrt(ok)*xx)/np.sqrt(ok)
    else:
        dk = xx
    return dk


# The distance modulus
def dist_mod(zs,om=0.3,w0=-1,wa=0.0, orr = 0.0):
    """ Calculate the distance modulus, correcting for curvature"""
    ok = 0
    xx = np.array([integrate.quad(ezinv, 0, z, args=(om, w0, wa, orr))[0] for z in zs])
    D = Sk(xx, ok)
    lum_dist = D * (1 + zs) 
    dist_mod = 5 * np.log10(lum_dist) # Distance modulus
    dist_mod = dist_mod + np.log(c/H0kmsmpc)-(-19.5)
    return dist_mod

n = 101                          # Increase this for a finer grid
oms = np.linspace(0, 1 , n)      # Array of matter densities
orrs = np.linspace(0, 1, n)      # Array of cosmological constant values
chi2 = np.ones((n, n)) * np.inf  # Array to hold our chi2 values, set initially to super large values

# Calculate Chi2 for each model
for i, om in enumerate(oms):                                           # Loop through matter densities
        for j, orr in enumerate(orrs):                                 # Loop through cosmological constant densities
            mu_model = dist_mod(zs,om=om, orr=orr)                     # Calculate the distance modulus vs redshift for that model 
            mscr = np.sum((mu_model-mu)/muerr**2)/np.sum(1./muerr**2)  # Calculate the vertical offset to apply
            mu_model_norm = mu_model-mscr                              # Apply the vertical offset
            chi2[i,j] = np.sum((mu_model_norm - mu) ** 2 / muerr**2)   # Calculate the chi2 and save it in a matrix
            
# Convert that to a likelihood and calculate the reduced chi2
likelihood = np.exp(-0.5 * (chi2-np.amin(chi2)))  
chi2_reduced = chi2 / (len(mu)-2)              

# Calculate the best fit values (where chi2 is minimum)
indbest = np.argmin(chi2)                 
ibest   = np.unravel_index(indbest,[n,n]) 
print( 'Best fit values are (om,orr)=(%.3f,%.3f)'%( oms[ibest[0]], orrs[ibest[1]] ) )
print( 'Reduced chi^2 for the best fit is %0.2f'%chi2_reduced[ibest[0],ibest[1]] )

# Contour plot in the O_m - O_r plane 
import matplotlib as mpl

# Define contour levels and colors
levels = [2.30, 6.18, 11.83]
colors = ['palevioletred', 'plum', 'black']

# Create colormap and norm
cmap = mpl.colors.ListedColormap(colors)
norm = mpl.colors.BoundaryNorm(levels, cmap.N)

# Plot contours
contours = plt.contour(oms, orrs, np.transpose(chi2-np.amin(chi2)), cmap=cmap, levels=levels)
plt.clabel(contours, inline=True, fontsize=10, fmt={levels[0]: r'$\sigma_{1}$', levels[1]: r'$\sigma_{2}$', levels[2]: r'$\sigma_{3}$'})

# Plot best fit point
plt.plot(oms[ibest[0]], orrs[ibest[1]], 'o', color='black', label='($\Omega_m$,$\Omega_R$)=(%.2f,%.2f)'%(oms[ibest[0]], orrs[ibest[1]]))

# Set axis labels and legend
plt.xlabel("$\Omega_m$", fontsize=12)
plt.ylabel("$\Omega_R$", fontsize=12)
plt.legend(frameon=False)

# Show the plot and close the figure
plt.show()
plt.close()
